{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONOsgQF03fT9Ca4ywpsDkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryaman1122/Stammer-Correction/blob/main/Stammer_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pydub\n",
        "!pip install gTTS\n",
        "!pip install SpeechRecognition\n",
        "!pip install -U openai-whisper openai"
      ],
      "metadata": {
        "id": "9YUI-QJytw4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ngrok\n",
        "!wget -q -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "M3DBJVJNtz-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken ENTER YOUR TOKEN"
      ],
      "metadata": {
        "id": "HLxnpwE4t6ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken <ENTER YOUR TOKEN>"
      ],
      "metadata": {
        "id": "y4XomyXTt9pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "metadata": {
        "id": "ZY4suUaEuFa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-ngrok\n",
        "!ngrok http 5000"
      ],
      "metadata": {
        "id": "h9tW48UjuFKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "iQXSJ7mhuNqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#pip install Flask torch librosa transformers gtts moviepy pyngrok\n",
        "import os\n",
        "from flask import Flask, request, render_template_string, send_from_directory\n",
        "from moviepy.editor import VideoFileClip\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from gtts import gTTS\n",
        "from pyngrok import ngrok  # REPLACEMENT for flask_ngrok\n",
        "\n",
        "# Set folders\n",
        "UPLOAD_FOLDER = 'uploads'\n",
        "OUTPUT_FOLDER = 'outputs'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "\n",
        "# Load Whisper model\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "model.eval()\n",
        "\n",
        "# Transcribe function\n",
        "def transcribe_audio(audio_file_path):\n",
        "    audio_input, _ = librosa.load(audio_file_path, sr=16000, mono=True)\n",
        "    inputs = processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(inputs[\"input_features\"], max_length=1024)\n",
        "    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "    return transcription\n",
        "\n",
        "# Dummy emotion detection\n",
        "def detect_emotion(transcribed_text):\n",
        "    emotions = [\"neutral\", \"happy\", \"sad\", \"angry\"]\n",
        "    return \"neutral\", 0.9  # Placeholder\n",
        "\n",
        "# Dummy stammer severity\n",
        "def detect_stammer_severity(audio_file_path):\n",
        "    return \"mild\"  # Placeholder\n",
        "\n",
        "# Convert to fluent audio\n",
        "def convert_to_fluent_audio(text, wav_file_path):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    mp3_file_path = os.path.join(OUTPUT_FOLDER, os.path.basename(wav_file_path).replace(\".wav\", \"_fluent.mp3\"))\n",
        "    tts.save(mp3_file_path)\n",
        "    return mp3_file_path\n",
        "\n",
        "# Home route\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template_string('''\n",
        "        <!doctype html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Speech Analyzer</title>\n",
        "            <style>\n",
        "                body { font-family: Arial; padding: 20px; }\n",
        "                .container { max-width: 600px; margin: auto; }\n",
        "                input[type=\"file\"] { margin: 10px 0; }\n",
        "                .btn { padding: 10px 15px; background-color: #4CAF50; color: white; border: none; cursor: pointer; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <h2>Upload Audio/Video for Analysis</h2>\n",
        "                <form action=\"/upload\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "                    <input type=\"file\" name=\"file\" required>\n",
        "                    <button type=\"submit\" class=\"btn\">Upload</button>\n",
        "                </form>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "    ''')\n",
        "\n",
        "# Serve output files\n",
        "@app.route('/outputs/<filename>')\n",
        "def download_file(filename):\n",
        "    return send_from_directory(OUTPUT_FOLDER, filename)\n",
        "\n",
        "# Upload route\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_audio():\n",
        "    if 'file' not in request.files:\n",
        "        return \"No file uploaded\"\n",
        "\n",
        "    file = request.files['file']\n",
        "    filename = file.filename\n",
        "    file_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "    file.save(file_path)\n",
        "\n",
        "    # Convert video to audio if needed\n",
        "    if filename.lower().endswith(('.mp4', '.mkv', '.avi', '.mov')):\n",
        "        audio_path = os.path.join(UPLOAD_FOLDER, filename.rsplit('.', 1)[0] + \".wav\")\n",
        "        clip = VideoFileClip(file_path)\n",
        "        clip.audio.write_audiofile(audio_path)\n",
        "        file_path = audio_path\n",
        "\n",
        "    # Ensure file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        return \"File conversion failed.\"\n",
        "\n",
        "    # Transcribe and analyze\n",
        "    audio_text = transcribe_audio(file_path)\n",
        "    emotion, score = detect_emotion(audio_text)\n",
        "    severity = detect_stammer_severity(file_path)\n",
        "    fluent_audio = convert_to_fluent_audio(audio_text, file_path)\n",
        "    fluent_audio_filename = os.path.basename(fluent_audio)\n",
        "\n",
        "    return render_template_string('''\n",
        "        <!doctype html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Results</title>\n",
        "            <style>\n",
        "                body { font-family: Arial; padding: 20px; }\n",
        "                .container { max-width: 700px; margin: auto; }\n",
        "                .btn { padding: 10px 15px; background-color: #008CBA; color: white; border: none; text-decoration: none; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <h2>Transcription Result</h2>\n",
        "                <p><strong>Text:</strong> {{ audio_text }}</p>\n",
        "                <p><strong>Detected Emotion:</strong> {{ emotion }} (Confidence: {{ confidence }})</p>\n",
        "                <p><strong>Stammer Severity:</strong> {{ severity }}</p>\n",
        "                <p><strong>Fluent Audio:</strong></p>\n",
        "                <audio controls>\n",
        "                    <source src=\"/outputs/{{ fluent_audio_filename }}\" type=\"audio/mpeg\">\n",
        "                    Your browser does not support the audio element.\n",
        "                </audio><br><br>\n",
        "                <a href=\"/outputs/{{ fluent_audio_filename }}\" class=\"btn\" download>Download Fluent Audio</a>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "    ''', audio_text=audio_text, emotion=emotion, confidence=round(score, 2),\n",
        "         severity=severity, fluent_audio_filename=fluent_audio_filename)\n",
        "\n",
        "# Start the app and ngrok tunnel\n",
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"\\n * ngrok tunnel available at: {public_url.public_url}\\n\")\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "t7UviQchuAeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}